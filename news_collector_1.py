"""
ë‰´ìŠ¤ ìˆ˜ì§‘ ëª¨ë“ˆ (News Collector)
================================
Google News RSS + trafilatura ë³¸ë¬¸ í¬ë¡¤ë§ ê¸°ë°˜
ì‚°ì—… íŠ¸ë Œë“œ / ê²½ìŸì‚¬ ë™í–¥ / ê·œì œ ë³€í™” 3ì¶• ìˆ˜ì§‘

ê¸°ëŠ¥:
- Google News RSSë¡œ ë‰´ìŠ¤ URL ìˆ˜ì§‘ (ë¬´ë£Œ, API í‚¤ ë¶ˆí•„ìš”)
- trafilaturaë¡œ ë³¸ë¬¸ ì „ì²´ í¬ë¡¤ë§ (ì •í™•ë„ F1 0.958)
- 3ê°œ ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜: ì‚°ì—… íŠ¸ë Œë“œ / ê²½ìŸì‚¬ ë™í–¥ / ê·œì œ ë³€í™”
- ì‚°ì—…ë³„, ê¸°ì—…ë³„, í‚¤ì›Œë“œë³„ ìˆ˜ì§‘ ì§€ì›
- ìˆ˜ì§‘ ê²°ê³¼ JSON/CSV ì €ì¥ + ìºì‹±

ì„¤ì¹˜:
    pip install requests trafilatura

ì‚¬ìš©ë²•:
    from news_collector import NewsCollector
    collector = NewsCollector()

    # ì‚°ì—…ë³„ ìˆ˜ì§‘
    news = collector.collect_industry_news("IT/ì†Œí”„íŠ¸ì›¨ì–´")

    # ê¸°ì—… ë§ì¶¤ ìˆ˜ì§‘ (ê²½ìŸì‚¬ í¬í•¨)
    news = collector.collect_for_company(
        company="ì‚¼ì„±ì „ì",
        industry="IT/ì†Œí”„íŠ¸ì›¨ì–´",
        competitors=["LGì „ì", "SKí•˜ì´ë‹‰ìŠ¤"]
    )
"""

import json
import re
import time
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from pathlib import Path
from dataclasses import dataclass, field, asdict
from typing import Optional
from urllib.parse import quote_plus

import requests

# trafilatura ë³¸ë¬¸ ì¶”ì¶œ (ì„¤ì¹˜ í•„ìš”)
try:
    import trafilatura
    HAS_TRAFILATURA = True
except ImportError:
    HAS_TRAFILATURA = False
    print("âš ï¸ trafilatura ë¯¸ì„¤ì¹˜. ë³¸ë¬¸ í¬ë¡¤ë§ ë¶ˆê°€. pip install trafilatura")


# ============================================================
# ë°ì´í„° ëª¨ë¸
# ============================================================

@dataclass
class NewsArticle:
    """ë‰´ìŠ¤ ê¸°ì‚¬ ë°ì´í„° ëª¨ë¸"""
    title: str = ""
    url: str = ""
    source: str = ""
    published_at: str = ""
    description: str = ""           # RSS ìš”ì•½
    full_text: str = ""             # ë³¸ë¬¸ ì „ì²´ (trafilatura)
    author: str = ""
    category: str = ""              # industry_trend / competitor / regulation
    category_label: str = ""        # ì‚°ì—… íŠ¸ë Œë“œ / ê²½ìŸì‚¬ ë™í–¥ / ê·œì œ ë³€í™”
    industry: str = ""
    company: str = ""               # ê´€ë ¨ ê¸°ì—…
    keywords: list = field(default_factory=list)
    word_count: int = 0
    crawl_success: bool = False
    crawled_at: str = ""


# ============================================================
# ë‰´ìŠ¤ ìˆ˜ì§‘ ì„¤ì •
# ============================================================

# ì‚°ì—…ë³„ ê²€ìƒ‰ í‚¤ì›Œë“œ â€” deta.kr 12ê°œ ì‚°ì—… ë¶„ë¥˜ ê¸°ì¤€ (ì˜ë¬¸ â€” í•´ì™¸ ë‰´ìŠ¤ ì†ŒìŠ¤ íƒ€ê²Ÿ)
INDUSTRY_CONFIG = {
    "í™”í•™ ë° ì¬ë£Œ": {
        "industry_trend": [
            "chemical industry trend 2026", "advanced materials innovation",
            "specialty chemicals market", "green chemistry", "polymer technology",
        ],
        "regulation": [
            "REACH regulation", "chemical safety regulation", "PFAS ban",
            "hazardous substance regulation", "carbon border adjustment",
        ],
        "competitor_keywords": ["chemical plant", "materials acquisition", "R&D investment", "partnership"],
    },
    "ì •ë³´í†µì‹ ê¸°ìˆ (ICT)": {
        "industry_trend": [
            "AI industry trend 2026", "enterprise SaaS market", "cloud transformation",
            "generative AI enterprise adoption", "software industry outlook",
        ],
        "regulation": [
            "AI regulation policy", "EU AI Act enforcement", "data protection law",
            "tech platform regulation", "digital privacy regulation",
        ],
        "competitor_keywords": ["funding", "product launch", "acquisition", "earnings"],
    },
    "ì „ì(ë°˜ë„ì²´ ë“±)": {
        "industry_trend": [
            "semiconductor market trend 2026", "chip manufacturing expansion",
            "AI chip demand", "display technology OLED", "consumer electronics outlook",
        ],
        "regulation": [
            "CHIPS Act", "semiconductor export control", "rare earth regulation",
            "electronics waste regulation", "trade restriction semiconductor",
        ],
        "competitor_keywords": ["fab construction", "chip revenue", "technology node", "foundry"],
    },
    "ìë™í™”": {
        "industry_trend": [
            "industrial automation trend 2026", "smart factory robotics",
            "Industry 4.0 adoption", "collaborative robot cobot", "manufacturing AI",
        ],
        "regulation": [
            "robot safety regulation", "industrial safety standard",
            "automation labor regulation", "machine directive EU",
        ],
        "competitor_keywords": ["factory expansion", "automation contract", "new technology", "partnership"],
    },
    "ìë™ì°¨": {
        "industry_trend": [
            "electric vehicle market 2026", "autonomous driving technology",
            "EV battery innovation", "connected car trend", "automotive supply chain",
        ],
        "regulation": [
            "EV subsidy policy", "emission regulation Euro 7", "autonomous vehicle regulation",
            "battery recycling mandate", "vehicle safety standard",
        ],
        "competitor_keywords": ["vehicle sales", "EV launch", "auto partnership", "factory investment"],
    },
    "ìš°ì£¼ ë° êµ­ë°©": {
        "industry_trend": [
            "space industry commercial 2026", "defense technology trend",
            "satellite constellation", "hypersonic technology", "space launch market",
        ],
        "regulation": [
            "defense procurement policy", "ITAR regulation", "space debris regulation",
            "arms export control", "dual use technology regulation",
        ],
        "competitor_keywords": ["defense contract", "satellite launch", "space funding", "military acquisition"],
    },
    "ì—ë„ˆì§€": {
        "industry_trend": [
            "renewable energy trend 2026", "hydrogen economy", "energy storage battery",
            "carbon capture technology", "nuclear energy revival",
        ],
        "regulation": [
            "carbon emission regulation", "renewable energy mandate", "ESG compliance",
            "energy transition policy", "carbon tax regulation",
        ],
        "competitor_keywords": ["energy project", "solar wind investment", "power plant", "clean energy funding"],
    },
    "ì‹ìŒë£Œ": {
        "industry_trend": [
            "food technology trend 2026", "alternative protein market",
            "food safety innovation", "beverage industry outlook", "sustainable packaging food",
        ],
        "regulation": [
            "food labeling regulation", "FDA food safety", "sugar tax policy",
            "food additive regulation", "organic certification standard",
        ],
        "competitor_keywords": ["food brand launch", "beverage acquisition", "F&B revenue", "restaurant chain"],
    },
    "ì†Œë¹„ì¬ ë° ì„œë¹„ìŠ¤": {
        "industry_trend": [
            "consumer goods trend 2026", "retail technology innovation",
            "D2C brand growth", "ecommerce market outlook", "luxury market trend",
        ],
        "regulation": [
            "consumer protection regulation", "ecommerce platform regulation",
            "product safety standard", "cross-border commerce regulation",
        ],
        "competitor_keywords": ["brand revenue", "retail expansion", "marketplace growth", "consumer spending"],
    },
    "ìƒëª…ê³¼í•™ ë° í—¬ìŠ¤ì¼€ì–´": {
        "industry_trend": [
            "digital health trend 2026", "biotech drug pipeline", "precision medicine",
            "AI in healthcare", "gene therapy advancement",
        ],
        "regulation": [
            "FDA approval drug", "medical device regulation", "clinical trial regulation",
            "health data privacy HIPAA", "telehealth regulation",
        ],
        "competitor_keywords": ["clinical trial results", "FDA approval", "biotech funding", "pharma acquisition"],
    },
    "êµìœ¡": {
        "industry_trend": [
            "edtech market trend 2026", "AI in education", "online learning platform",
            "corporate training technology", "education technology innovation",
        ],
        "regulation": [
            "education data privacy", "AI education regulation", "online learning accreditation",
            "student data protection FERPA",
        ],
        "competitor_keywords": ["edtech funding", "education platform launch", "university partnership", "LMS"],
    },
    "ë†ì—…": {
        "industry_trend": [
            "agritech trend 2026", "precision agriculture", "smart farming technology",
            "agricultural drone", "vertical farming market",
        ],
        "regulation": [
            "agricultural subsidy policy", "pesticide regulation", "GMO regulation",
            "sustainable agriculture standard", "food supply chain regulation",
        ],
        "competitor_keywords": ["agritech investment", "farm equipment", "crop technology", "agriculture acquisition"],
    },
    "ê¸°íƒ€": {
        "industry_trend": [
            "global business trend 2026", "digital transformation", "industry outlook",
        ],
        "regulation": [
            "corporate regulation change", "ESG regulation", "antitrust regulation",
        ],
        "competitor_keywords": ["growth", "investment", "innovation"],
    },
}

# í•œêµ­ ì¶œì²˜ í•„í„°ë§ ë¦¬ìŠ¤íŠ¸ (ì œì™¸ ëŒ€ìƒ)
KOREAN_SOURCE_PATTERNS = [
    # í•œêµ­ ë„ë©”ì¸
    ".kr", "daum.net", "naver.com", "chosun.com", "joongang.co",
    "donga.com", "hankyung.com", "mk.co", "sedaily.com", "etnews.com",
    "zdnet.co.kr", "bloter.net", "platum.kr", "venturesquare.net",
    "aitimes.com", "aitimes.kr", "techm.kr", "byline.network",
    # í•œêµ­ ì†ŒìŠ¤ëª…
    "ì¡°ì„ ì¼ë³´", "ì¤‘ì•™ì¼ë³´", "ë™ì•„ì¼ë³´", "í•œêµ­ê²½ì œ", "ë§¤ì¼ê²½ì œ",
    "ì„œìš¸ê²½ì œ", "ì „ìì‹ ë¬¸", "ì§€ë””ë„·ì½”ë¦¬ì•„", "ë¸”ë¡œí„°", "í”Œë˜í…€",
    "ë²¤ì²˜ìŠ¤í€˜ì–´", "AIíƒ€ì„ìŠ¤", "í…Œí¬ì— ", "ë°”ì´ë¼ì¸ë„¤íŠ¸ì›Œí¬",
    "ì—°í•©ë‰´ìŠ¤", "KBS", "MBC", "SBS", "JTBC", "YTN",
    "v.daum.net", "news.naver.com", "n.news.naver.com",
    "Vietnam.vn",  # ë² íŠ¸ë‚¨ì–´ ì†ŒìŠ¤ë„ ì œì™¸
]


# ============================================================
# Google News RSS ìˆ˜ì§‘ê¸°
# ============================================================

class GoogleNewsRSS:
    """Google News RSS í”¼ë“œ ìˆ˜ì§‘ê¸°"""

    RSS_URL = "https://news.google.com/rss/search"
    HEADERS = {
        "User-Agent": "Mozilla/5.0 (compatible; NewsBot/1.0)",
    }

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update(self.HEADERS)

    def search(
        self,
        query: str,
        lang: str = "en",
        country: str = "US",
        max_results: int = 5,
        days: int = 7,
        exclude_korean: bool = True,
    ) -> list[dict]:
        """
        Google News RSS ê²€ìƒ‰ (ê¸°ë³¸: ì˜ë¬¸/ë¯¸êµ­ â€” í•´ì™¸ ì†ŒìŠ¤)

        Args:
            query: ê²€ìƒ‰ì–´
            lang: ì–¸ì–´ ì½”ë“œ (ê¸°ë³¸: en)
            country: êµ­ê°€ ì½”ë“œ (ê¸°ë³¸: US)
            max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜
            days: ìµœê·¼ Nì¼ ì´ë‚´
            exclude_korean: í•œêµ­ ì¶œì²˜ ì œì™¸ ì—¬ë¶€ (ê¸°ë³¸: True)
        """
        params = {
            "q": query,
            "hl": lang,
            "gl": country,
            "ceid": f"{country}:{lang}",
        }

        # ê¸°ê°„ í•„í„° (Google News when: íŒŒë¼ë¯¸í„°)
        if days <= 1:
            params["q"] += " when:1d"
        elif days <= 7:
            params["q"] += " when:7d"
        elif days <= 30:
            params["q"] += " when:30d"

        try:
            resp = self.session.get(self.RSS_URL, params=params, timeout=15)
            if resp.status_code != 200:
                print(f"  âš ï¸ RSS ìš”ì²­ ì‹¤íŒ¨ ({resp.status_code})")
                return []

            root = ET.fromstring(resp.content)
            items = root.findall(".//item")
            results = []

            # í•œêµ­ ì†ŒìŠ¤ í•„í„°ë§ ì‹œ ì¶©ë¶„íˆ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ ì—¬ìœ ë¶„ í™•ë³´
            fetch_limit = max_results * 3 if exclude_korean else max_results

            for item in items[:fetch_limit]:
                title = item.findtext("title", "")
                link = item.findtext("link", "")
                pub_date = item.findtext("pubDate", "")
                source = item.findtext("source", "")
                desc = item.findtext("description", "")
                desc_clean = re.sub(r"<[^>]+>", "", desc) if desc else ""

                # í•œêµ­ ì¶œì²˜ í•„í„°ë§
                if exclude_korean and self._is_korean_source(source, link, title):
                    continue

                results.append({
                    "title": title,
                    "url": link,
                    "source": source,
                    "published_at": pub_date,
                    "description": desc_clean[:500],
                })

                if len(results) >= max_results:
                    break

            return results

        except Exception as e:
            print(f"  âš ï¸ RSS ì˜¤ë¥˜: {e}")
            return []

    @staticmethod
    def _is_korean_source(source: str, url: str, title: str) -> bool:
        """í•œêµ­ ì¶œì²˜ì¸ì§€ íŒë³„"""
        check_text = f"{source} {url} {title}".lower()
        for pattern in KOREAN_SOURCE_PATTERNS:
            if pattern.lower() in check_text:
                return True
        # í•œê¸€ ë¬¸ì ë¹„ìœ¨ ì²´í¬ (ì œëª©ì— í•œê¸€ì´ 50% ì´ìƒì´ë©´ í•œêµ­ ì†ŒìŠ¤)
        if title:
            korean_chars = len(re.findall(r'[ê°€-í£]', title))
            if korean_chars > len(title) * 0.3:
                return True
        return False


# ============================================================
# ë³¸ë¬¸ í¬ë¡¤ëŸ¬ (trafilatura)
# ============================================================

class ArticleCrawler:
    """trafilatura ê¸°ë°˜ ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ëŸ¬"""

    HEADERS = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                      "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9",
        "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.8",
    }

    def __init__(self, timeout: int = 15):
        self.timeout = timeout
        self.session = requests.Session()
        self.session.headers.update(self.HEADERS)
        self._crawled_count = 0

    def extract_article(self, url: str) -> dict:
        """
        URLì—ì„œ ê¸°ì‚¬ ë³¸ë¬¸ + ë©”íƒ€ë°ì´í„° ì¶”ì¶œ

        Returns:
            {
                "full_text": "ë³¸ë¬¸ í…ìŠ¤íŠ¸",
                "title": "ì œëª©",
                "author": "ì €ì",
                "date": "ë‚ ì§œ",
                "word_count": 1234,
                "success": True/False,
            }
        """
        if not HAS_TRAFILATURA:
            return {"full_text": "", "success": False, "error": "trafilatura ë¯¸ì„¤ì¹˜"}

        try:
            # Google News ë¦¬ë‹¤ì´ë ‰íŠ¸ URL ì²˜ë¦¬
            actual_url = self._resolve_google_news_url(url)

            # HTML ë‹¤ìš´ë¡œë“œ
            downloaded = trafilatura.fetch_url(actual_url)
            if not downloaded:
                # ì§ì ‘ requestsë¡œ ì‹œë„
                resp = self.session.get(actual_url, timeout=self.timeout, allow_redirects=True)
                if resp.status_code == 200:
                    downloaded = resp.text
                else:
                    return {"full_text": "", "success": False, "error": f"HTTP {resp.status_code}"}

            # ë³¸ë¬¸ ì¶”ì¶œ
            result = trafilatura.extract(
                downloaded,
                include_comments=False,
                include_tables=False,
                include_links=False,
                include_images=False,
                output_format="txt",
                favor_precision=True,  # ì •ë°€ë„ ìš°ì„ 
            )

            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            metadata = trafilatura.extract_metadata(downloaded)

            if result and len(result) > 100:  # ìµœì†Œ 100ì ì´ìƒì´ë©´ ì„±ê³µ
                self._crawled_count += 1
                return {
                    "full_text": result,
                    "title": metadata.title if metadata else "",
                    "author": metadata.author if metadata else "",
                    "date": str(metadata.date) if metadata and metadata.date else "",
                    "word_count": len(result),
                    "success": True,
                }
            else:
                return {"full_text": result or "", "success": False, "error": "ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ (ë‚´ìš© ë¶€ì¡±)"}

        except Exception as e:
            return {"full_text": "", "success": False, "error": str(e)}

    def _resolve_google_news_url(self, url: str) -> str:
        """Google News ë¦¬ë‹¤ì´ë ‰íŠ¸ URLì„ ì‹¤ì œ URLë¡œ ë³€í™˜"""
        if "news.google.com" in url:
            try:
                resp = self.session.head(url, allow_redirects=True, timeout=10)
                return resp.url
            except Exception:
                return url
        return url

    def get_crawled_count(self) -> int:
        return self._crawled_count


# ============================================================
# ë‰´ìŠ¤ ë¶„ë¥˜ê¸°
# ============================================================

class NewsClassifier:
    """ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜"""

    REGULATION_KEYWORDS = [
        "ê·œì œ", "ë²•ì•ˆ", "ë²•ë¥ ", "ì˜ë¬´í™”", "í—ˆê°€", "ì¸í—ˆê°€", "ê¸ˆì§€",
        "ê³¼ì§•ê¸ˆ", "ì œì¬", "ì¤€ìˆ˜", "ì»´í”Œë¼ì´ì–¸ìŠ¤", "ê°ë…", "ê°ì‚¬",
        "regulation", "compliance", "ban", "mandate", "policy",
        "ê°œì •", "ì‹œí–‰", "ìœ„ë°˜", "ì²˜ë²Œ", "ê°€ì´ë“œë¼ì¸",
    ]

    COMPETITOR_KEYWORDS = [
        "ì‹¤ì ", "ë§¤ì¶œ", "ì˜ì—…ì´ìµ", "íˆ¬ì", "ì¸ìˆ˜", "í•©ë³‘", "M&A",
        "ì¶œì‹œ", "ëŸ°ì¹­", "ì„œë¹„ìŠ¤ ì‹œì‘", "ì œíœ´", "íŒŒíŠ¸ë„ˆì‹­", "í˜‘ì—…",
        "IPO", "ìƒì¥", "ìœ ì¹˜", "í™•ì¥", "ì§„ì¶œ", "ì±„ìš©",
        "revenue", "acquisition", "launch", "partnership",
    ]

    TREND_KEYWORDS = [
        "íŠ¸ë Œë“œ", "ì „ë§", "ì„±ì¥", "í˜ì‹ ", "ë¯¸ë˜", "ë³€í™”", "ë™í–¥",
        "ì‹œì¥", "ë¶„ì„", "ë³´ê³ ì„œ", "ë¦¬í¬íŠ¸", "ì˜ˆì¸¡", "í™•ëŒ€",
        "trend", "forecast", "market", "growth", "innovation",
        "ì „í™˜", "ë„ì…", "ë¶€ìƒ", "ì£¼ëª©",
    ]

    def classify(self, article: NewsArticle) -> NewsArticle:
        """ê¸°ì‚¬ ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜"""
        text = f"{article.title} {article.description} {article.full_text[:500]}"
        text_lower = text.lower()

        reg_score = sum(1 for kw in self.REGULATION_KEYWORDS if kw in text_lower)
        comp_score = sum(1 for kw in self.COMPETITOR_KEYWORDS if kw in text_lower)
        trend_score = sum(1 for kw in self.TREND_KEYWORDS if kw in text_lower)

        if reg_score > comp_score and reg_score > trend_score:
            article.category = "regulation"
            article.category_label = "ê·œì œ ë³€í™”"
        elif comp_score > trend_score:
            article.category = "competitor"
            article.category_label = "ê²½ìŸì‚¬ ë™í–¥"
        else:
            article.category = "industry_trend"
            article.category_label = "ì‚°ì—… íŠ¸ë Œë“œ"

        return article

    def extract_keywords(self, text: str, top_n: int = 5) -> list[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨ ë¹ˆë„ ê¸°ë°˜)"""
        # 2ê¸€ì ì´ìƒ í•œê¸€ ë‹¨ì–´ ì¶”ì¶œ
        words = re.findall(r'[ê°€-í£]{2,}', text)
        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = {"ê²ƒì´", "í•˜ëŠ”", "ìˆëŠ”", "ì´ë²ˆ", "ëŒ€í•œ", "í†µí•´", "ìœ„í•´", "ì—ì„œ",
                     "ìœ¼ë¡œ", "ê¹Œì§€", "ë¶€í„°", "ì—ê²Œ", "ì´ë¼", "í•˜ê³ ", "ë˜ëŠ”", "í–ˆë‹¤",
                     "í•œë‹¤", "ìˆë‹¤", "ëœë‹¤", "ì´ë‹¤", "ë¼ê³ ", "ì—ëŠ”"}
        words = [w for w in words if w not in stopwords and len(w) >= 2]

        freq = {}
        for w in words:
            freq[w] = freq.get(w, 0) + 1
        sorted_words = sorted(freq.items(), key=lambda x: x[1], reverse=True)
        return [w for w, c in sorted_words[:top_n]]


# ============================================================
# ë©”ì¸ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°
# ============================================================

class NewsCollector:
    """ë‰´ìŠ¤ ìˆ˜ì§‘ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°"""

    def __init__(self, crawl_body: bool = True, cache_dir: str = "output/news_cache"):
        """
        Args:
            crawl_body: ë³¸ë¬¸ í¬ë¡¤ë§ ì—¬ë¶€ (Falseë©´ ì œëª©+ìš”ì•½ë§Œ)
            cache_dir: ìºì‹œ ë””ë ‰í† ë¦¬
        """
        self.rss = GoogleNewsRSS()
        self.crawler = ArticleCrawler() if crawl_body and HAS_TRAFILATURA else None
        self.classifier = NewsClassifier()
        self.crawl_body = crawl_body and HAS_TRAFILATURA
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)

    def collect_industry_news(
        self,
        industry: str,
        days: int = 7,
        max_per_category: int = 3,
    ) -> list[NewsArticle]:
        """
        ì‚°ì—…ë³„ ë‰´ìŠ¤ ìˆ˜ì§‘ (3ê°œ ì¹´í…Œê³ ë¦¬)

        Args:
            industry: ì‚°ì—… ë¶„ë¥˜ëª… (INDUSTRY_CONFIG í‚¤)
            days: ìµœê·¼ Nì¼
            max_per_category: ì¹´í…Œê³ ë¦¬ë‹¹ ìµœëŒ€ ê¸°ì‚¬ ìˆ˜
        """
        config = INDUSTRY_CONFIG.get(industry, INDUSTRY_CONFIG["ê¸°íƒ€"])
        all_articles = []

        print(f"\n{'='*50}")
        print(f"ğŸ“° {industry} ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")
        print(f"{'='*50}")

        # 1) ì‚°ì—… íŠ¸ë Œë“œ
        print(f"\nğŸ” [ì‚°ì—… íŠ¸ë Œë“œ] ìˆ˜ì§‘ ì¤‘...")
        trend_kws = config["industry_trend"]
        for kw in trend_kws[:3]:  # ìƒìœ„ 3ê°œ í‚¤ì›Œë“œ
            results = self.rss.search(kw, max_results=2, days=days)
            for r in results:
                article = self._process_result(r, industry, "industry_trend", "ì‚°ì—… íŠ¸ë Œë“œ")
                if article:
                    all_articles.append(article)
            time.sleep(0.5)

        # 2) ê·œì œ ë³€í™”
        print(f"\nğŸ” [ê·œì œ ë³€í™”] ìˆ˜ì§‘ ì¤‘...")
        reg_kws = config["regulation"]
        for kw in reg_kws[:3]:
            results = self.rss.search(kw, max_results=2, days=days)
            for r in results:
                article = self._process_result(r, industry, "regulation", "ê·œì œ ë³€í™”")
                if article:
                    all_articles.append(article)
            time.sleep(0.5)

        # ì¤‘ë³µ ì œê±° + ì¹´í…Œê³ ë¦¬ë³„ ì œí•œ
        all_articles = self._deduplicate(all_articles)
        all_articles = self._limit_per_category(all_articles, max_per_category)

        print(f"\nâœ… {industry} ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: {len(all_articles)}ê±´")
        self._print_summary(all_articles)
        return all_articles

    def collect_competitor_news(
        self,
        competitors: list[str],
        industry: str = "ê¸°íƒ€",
        days: int = 14,
        max_per_company: int = 3,
    ) -> list[NewsArticle]:
        """
        ê²½ìŸì‚¬ ë™í–¥ ë‰´ìŠ¤ ìˆ˜ì§‘

        Args:
            competitors: ê²½ìŸì‚¬ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
            industry: ì‚°ì—… ë¶„ë¥˜
            days: ìµœê·¼ Nì¼
            max_per_company: ê²½ìŸì‚¬ë‹¹ ìµœëŒ€ ê¸°ì‚¬ ìˆ˜
        """
        config = INDUSTRY_CONFIG.get(industry, INDUSTRY_CONFIG["ê¸°íƒ€"])
        comp_kws = config.get("competitor_keywords", ["íˆ¬ì", "ì¶œì‹œ", "ì‹¤ì "])
        all_articles = []

        print(f"\n{'='*50}")
        print(f"ğŸ¢ ê²½ìŸì‚¬ ë™í–¥ ìˆ˜ì§‘ ({len(competitors)}ê°œì‚¬)")
        print(f"{'='*50}")

        for company in competitors:
            print(f"\nğŸ” [{company}] ìˆ˜ì§‘ ì¤‘...")
            # íšŒì‚¬ëª… ë‹¨ë… ê²€ìƒ‰
            results = self.rss.search(company, max_results=3, days=days)
            for r in results:
                article = self._process_result(r, industry, "competitor", "ê²½ìŸì‚¬ ë™í–¥")
                if article:
                    article.company = company
                    all_articles.append(article)

            # íšŒì‚¬ëª… + í‚¤ì›Œë“œ ì¡°í•© ê²€ìƒ‰
            for kw in comp_kws[:2]:
                results = self.rss.search(f"{company} {kw}", max_results=2, days=days)
                for r in results:
                    article = self._process_result(r, industry, "competitor", "ê²½ìŸì‚¬ ë™í–¥")
                    if article:
                        article.company = company
                        all_articles.append(article)
            time.sleep(0.5)

        all_articles = self._deduplicate(all_articles)

        # ê²½ìŸì‚¬ë³„ ì œí•œ
        by_company = {}
        for a in all_articles:
            by_company.setdefault(a.company, []).append(a)
        limited = []
        for comp, arts in by_company.items():
            limited.extend(arts[:max_per_company])

        print(f"\nâœ… ê²½ìŸì‚¬ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: {len(limited)}ê±´")
        return limited

    def collect_for_company(
        self,
        company: str,
        industry: str,
        competitors: list[str] = None,
        days: int = 7,
        max_per_category: int = 3,
    ) -> dict:
        """
        íŠ¹ì • ê¸°ì—… ë§ì¶¤í˜• ë‰´ìŠ¤ ìˆ˜ì§‘ (3ì¶• í†µí•©)

        Returns:
            {
                "industry_trend": [NewsArticle, ...],
                "competitor": [NewsArticle, ...],
                "regulation": [NewsArticle, ...],
                "company_news": [NewsArticle, ...],
                "all": [NewsArticle, ...],
            }
        """
        print(f"\n{'='*60}")
        print(f"ğŸ¯ {company} ë§ì¶¤í˜• ë‰´ìŠ¤ ìˆ˜ì§‘")
        print(f"   ì‚°ì—…: {industry}")
        print(f"   ê²½ìŸì‚¬: {competitors or 'ì—†ìŒ'}")
        print(f"{'='*60}")

        result = {
            "industry_trend": [],
            "competitor": [],
            "regulation": [],
            "company_news": [],
            "all": [],
        }

        # 1) ì‚°ì—… íŠ¸ë Œë“œ + ê·œì œ ë³€í™”
        industry_news = self.collect_industry_news(industry, days, max_per_category)
        for a in industry_news:
            result[a.category].append(a)

        # 2) ê²½ìŸì‚¬ ë™í–¥
        if competitors:
            comp_news = self.collect_competitor_news(competitors, industry, days * 2)
            result["competitor"].extend(comp_news)

        # 3) íƒ€ê²Ÿ ê¸°ì—… ìì²´ ë‰´ìŠ¤
        print(f"\nğŸ” [{company}] ìì²´ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
        company_results = self.rss.search(company, max_results=5, days=days)
        for r in company_results:
            article = self._process_result(r, industry, "company", "ê¸°ì—… ë‰´ìŠ¤")
            if article:
                article.company = company
                result["company_news"].append(article)

        # ì „ì²´ í•©ì‚°
        for key in ["industry_trend", "competitor", "regulation", "company_news"]:
            result["all"].extend(result[key])

        print(f"\n{'='*60}")
        print(f"ğŸ“Š {company} ë§ì¶¤ ë‰´ìŠ¤ ìˆ˜ì§‘ ê²°ê³¼")
        print(f"   ì‚°ì—… íŠ¸ë Œë“œ: {len(result['industry_trend'])}ê±´")
        print(f"   ê²½ìŸì‚¬ ë™í–¥: {len(result['competitor'])}ê±´")
        print(f"   ê·œì œ ë³€í™”: {len(result['regulation'])}ê±´")
        print(f"   ê¸°ì—… ë‰´ìŠ¤: {len(result['company_news'])}ê±´")
        print(f"   ì´: {len(result['all'])}ê±´")
        print(f"{'='*60}")

        return result

    # ----------------------------------------------------------
    # ë‚´ë¶€ ë©”ì„œë“œ
    # ----------------------------------------------------------

    def _process_result(self, rss_item: dict, industry: str, category: str, category_label: str) -> Optional[NewsArticle]:
        """RSS ê²°ê³¼ â†’ NewsArticle ë³€í™˜ + ë³¸ë¬¸ í¬ë¡¤ë§"""
        url = rss_item.get("url", "")
        if not url:
            return None

        article = NewsArticle(
            title=rss_item.get("title", ""),
            url=url,
            source=rss_item.get("source", ""),
            published_at=rss_item.get("published_at", ""),
            description=rss_item.get("description", ""),
            category=category,
            category_label=category_label,
            industry=industry,
            crawled_at=datetime.now().isoformat(),
        )

        # ë³¸ë¬¸ í¬ë¡¤ë§
        if self.crawl_body and self.crawler:
            extracted = self.crawler.extract_article(url)
            if extracted.get("success"):
                article.full_text = extracted["full_text"]
                article.author = extracted.get("author", "")
                article.word_count = extracted.get("word_count", 0)
                article.crawl_success = True
                # í‚¤ì›Œë“œ ì¶”ì¶œ
                article.keywords = self.classifier.extract_keywords(article.full_text)
                print(f"  ğŸ“„ í¬ë¡¤ë§ ì„±ê³µ: {article.title[:40]}... ({article.word_count}ì)")
            else:
                article.crawl_success = False
                print(f"  âš ï¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {article.title[:40]}... ({extracted.get('error', '')})")
        else:
            article.crawl_success = False

        # ìë™ ë¶„ë¥˜ (ì¹´í…Œê³ ë¦¬ê°€ ë¯¸ì§€ì •ì´ê±°ë‚˜ ì¬ë¶„ë¥˜ í•„ìš” ì‹œ)
        if article.full_text or article.description:
            article = self.classifier.classify(article)

        return article

    def _deduplicate(self, articles: list[NewsArticle]) -> list[NewsArticle]:
        """URL ê¸°ë°˜ ì¤‘ë³µ ì œê±°"""
        seen_urls = set()
        unique = []
        for a in articles:
            normalized = a.url.split("?")[0].rstrip("/")
            if normalized not in seen_urls:
                seen_urls.add(normalized)
                unique.append(a)
        return unique

    def _limit_per_category(self, articles: list[NewsArticle], max_per: int) -> list[NewsArticle]:
        """ì¹´í…Œê³ ë¦¬ë³„ ê¸°ì‚¬ ìˆ˜ ì œí•œ"""
        by_cat = {}
        for a in articles:
            by_cat.setdefault(a.category, []).append(a)
        result = []
        for cat, arts in by_cat.items():
            result.extend(arts[:max_per])
        return result

    def _print_summary(self, articles: list[NewsArticle]):
        """ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        by_cat = {}
        for a in articles:
            by_cat.setdefault(a.category_label, []).append(a)
        for cat, arts in by_cat.items():
            crawled = sum(1 for a in arts if a.crawl_success)
            print(f"  {cat}: {len(arts)}ê±´ (ë³¸ë¬¸ í¬ë¡¤ë§: {crawled}ê±´)")

    # ----------------------------------------------------------
    # ì €ì¥ / ë¡œë“œ
    # ----------------------------------------------------------

    def save_articles(self, articles: list[NewsArticle], filepath: str = ""):
        """ìˆ˜ì§‘ ê²°ê³¼ JSON ì €ì¥"""
        if not filepath:
            ts = datetime.now().strftime("%Y%m%d_%H%M")
            filepath = f"output/news_{ts}.json"

        fp = Path(filepath)
        fp.parent.mkdir(parents=True, exist_ok=True)

        data = [asdict(a) for a in articles]
        with open(fp, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"âœ… ë‰´ìŠ¤ ì €ì¥: {fp} ({len(articles)}ê±´)")

    @staticmethod
    def load_articles(filepath: str) -> list[NewsArticle]:
        """ì €ì¥ëœ ë‰´ìŠ¤ ë¡œë“œ"""
        with open(filepath, "r", encoding="utf-8") as f:
            data = json.load(f)
        return [NewsArticle(**item) for item in data]


# ============================================================
# ì‹¤í–‰ ì˜ˆì‹œ
# ============================================================

if __name__ == "__main__":
    collector = NewsCollector(crawl_body=True)

    # ---- ì˜ˆì‹œ 1: ì‚°ì—…ë³„ ë‰´ìŠ¤ ìˆ˜ì§‘ ----
    # news = collector.collect_industry_news("IT/ì†Œí”„íŠ¸ì›¨ì–´", days=7)
    # collector.save_articles(news)

    # ---- ì˜ˆì‹œ 2: ê²½ìŸì‚¬ ë™í–¥ ìˆ˜ì§‘ ----
    # news = collector.collect_competitor_news(
    #     competitors=["ë„¤ì´ë²„", "ì¹´ì¹´ì˜¤", "ì¿ íŒ¡"],
    #     industry="IT/ì†Œí”„íŠ¸ì›¨ì–´",
    # )

    # ---- ì˜ˆì‹œ 3: ê¸°ì—… ë§ì¶¤í˜• í†µí•© ìˆ˜ì§‘ (ì¶”ì²œ) ----
    result = collector.collect_for_company(
        company="ì‚¼ì„±ì „ì",
        industry="IT/ì†Œí”„íŠ¸ì›¨ì–´",
        competitors=["LGì „ì", "SKí•˜ì´ë‹‰ìŠ¤", "TSMC"],
        days=7,
    )

    # ì „ì²´ ì €ì¥
    collector.save_articles(result["all"])

    # ì¹´í…Œê³ ë¦¬ë³„ í™•ì¸
    for article in result["all"][:5]:
        print(f"\n[{article.category_label}] {article.title}")
        print(f"  ì¶œì²˜: {article.source} | ë³¸ë¬¸: {article.word_count}ì")
        if article.keywords:
            print(f"  í‚¤ì›Œë“œ: {', '.join(article.keywords)}")
